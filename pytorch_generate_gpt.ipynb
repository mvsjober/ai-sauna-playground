{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e6e42c-a2a3-4ba9-be85-056035147486",
   "metadata": {},
   "source": [
    "# Play around with generating text from an LLM\n",
    "\n",
    "First, let's check if we have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb14ca-2fa0-4ea4-b423-0d06f58b57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab1638-49a7-4bab-acde-4b6710830809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249065c-17ee-48cf-93ae-21798dc271c3",
   "metadata": {},
   "source": [
    "Let's change the path for the Huggingface cache where it stores the models it downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f4d67-1376-42b8-a7d3-4cf65ad4b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/project_462000584/huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b81ba4-2bc1-4164-8304-127eff870e6a",
   "metadata": {},
   "source": [
    "We'll create a helper function for printing the generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27363041-16c2-4448-98e3-f10853ed8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    for item in output:\n",
    "        text = item['generated_text']\n",
    "        text = text.replace(\"<br />\", \"\\n\")\n",
    "        print('-', text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7f8cb-5457-4c4e-9d5d-317bf58b9c8e",
   "metadata": {},
   "source": [
    "We'll create a generator object using the Huggingface Transformers pipeline.\n",
    "\n",
    "Note that you have to select what model to use from the Huggingface model repository: https://huggingface.co/models\n",
    "\n",
    "You can change the model by modifying the `model_name` variable. For example \"distilgpt2\" is a light and fast model similar to GPT-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f197f-ad49-432e-9180-de5a5d278134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#model_name = 'distilgpt2'\n",
    "#model_name = 'microsoft/phi-2'\n",
    "model_name = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665e215-1ee7-43fc-9403-a585de3f0b2d",
   "metadata": {},
   "source": [
    "Finally, let's try generating an output based on a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcaa188-5708-4eca-a3a5-4caa841748e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(\"The AI Sauna\")\n",
    "print_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb8536-887f-4fad-a0b3-190d1749a594",
   "metadata": {},
   "source": [
    "## Experiment with the generation strategy\n",
    "\n",
    "You can play with the text generation if you wish. Text generation strategies are discussed here: https://huggingface.co/docs/transformers/generation_strategies\n",
    "\n",
    "Note that we are here using the easy-to-use `TextGenerationPipeline` and its `generator()` function, but the link discusses the `model.generate()` method. The same parameters can be used, though, the pipeline just takes care of some of the pre- and post-processing.\n",
    "\n",
    "In particular these parameters of the `generator()` function might be interesting:\n",
    "\n",
    "- `max_new_tokens`: the maximum number of tokens to generate\n",
    "- `num_beams`: activate Beam search by setting this > 1\n",
    "- `do_sample`: activate multinomial sampling if set to True\n",
    "- `num_return_sequences`: the number of candidate sentences to return (available only for beam search and sampling)\n",
    "\n",
    "Here is a nice blog post explaining in more detail about the different generation strategies: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816b3f3-9a0f-4ca8-a7d9-d7962b0207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(\"What is the AI Sauna event about?\", num_return_sequences=1, max_new_tokens=100)\n",
    "print_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62279a12-140a-4579-9fee-d1cc1b4e8ff2",
   "metadata": {},
   "source": [
    "# Generate images with Stable Diffusion XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dee1d5-a587-4716-a913-d7bae84991ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1d639-41dc-4e4c-8120-43224675ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03314df2-2bfe-4ef2-b964-7d7164c653c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photo from the AI sauna hackathon\"\n",
    "image = pipe(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7d9a5-bc14-4246-9d37-744f40a6b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(\"generated_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3a30b-e570-4e83-ab85-e09aeca22be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
